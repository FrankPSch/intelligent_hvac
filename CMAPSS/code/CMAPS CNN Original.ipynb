{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Approach using MLP. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, pickle, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler,EarlyStopping\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.pooling import AveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation, Input, merge, Conv2D, Reshape, Flatten, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras\n",
    "from sklearn import preprocessing\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import CMAPSAuxFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureN = 14\n",
    "nb_epoch = 250\n",
    "batch_size = 512\n",
    "FilterN = 10\n",
    "FilterL = 10\n",
    "rmse,sco,tm = [], [], []\n",
    "\n",
    "ConstRUL = 125\n",
    "TW = 30\n",
    "Dataset = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the train and test data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seed = 2222\\nnp.random.seed(seed)\\nnp.random.shuffle(samples)\\nnp.random.seed(seed)\\nnp.random.shuffle(targets)\\nsamplet = samplet[np.argsort(labelt)]\\nlabelt = labelt[np.argsort(labelt)]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############ training samples ##################################\n",
    "\n",
    "setTrain = {'1':100, '2':260, '3':100, '4':248}\n",
    "setTest = {'1':100, '2':259, '3':100, '4':248}\n",
    "nTrain = setTrain[Dataset]\n",
    "nTest = setTest[Dataset]\n",
    "\n",
    "data = [] \n",
    "for line in open(\"../CMAPSSData/train_FD00\"+Dataset+\".txt\"):\n",
    "    data.append(line.split())\n",
    "data=np.array(data)\n",
    "data = np.cast['float64'](data)\n",
    "data_copy = data\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data = min_max_scaler.fit_transform(data)   #Why scale the data with all of the \"missing\" rows.\n",
    "num=[]\n",
    "for i in range(nTrain):\n",
    "    tmp = data[np.where(data_copy[:,0]==i+1),:][0][:, np.array([6,7,8,11,12,13,15,16,17,18,19,21,24,25])]\n",
    "    num.append(tmp)\n",
    "num=np.array(num)\n",
    "\n",
    "label=[]\n",
    "for i in range(nTrain):\n",
    "    label.append([])\n",
    "    length = len(num[i])\n",
    "    for j in range(length):\n",
    "        label[i].append(ConstRUL if length-j-1>=ConstRUL else length-j-1)\n",
    "label = np.array(label)\n",
    "\n",
    "samples,targets,noofsample = [],[],[]\n",
    "for i in range(nTrain):\n",
    "    noofsample.append(len(num[i])-TW+1)\n",
    "    for j in range(noofsample[-1]):\n",
    "        samples.append(num[i][j:j+TW,:])\n",
    "        targets.append(label[i][j+TW-1])\n",
    "samples = np.array(samples)\n",
    "targets = np.array(targets)\n",
    "\n",
    "################## testing data ###########################\n",
    "data = [] \n",
    "for line in open(\"../CMAPSSData/test_FD00\"+Dataset+\".txt\"):\n",
    "    data.append(line.split())\n",
    "data=np.array(data)\n",
    "data = np.cast['float64'](data)\n",
    "data_copy = data\n",
    "data = min_max_scaler.transform(data)  #Why scale the data with all of the \"missing\" rows.\n",
    "numt=[]\n",
    "for i in range(nTest):\n",
    "    tmp = data[np.where(data_copy[:,0]==i+1),:][0][:, np.array([6,7,8,11,12,13,15,16,17,18,19,21,24,25])]\n",
    "    numt.append(tmp)\n",
    "numt=np.array(numt)\n",
    "\n",
    "samplet, count_miss = [],[]\n",
    "for i in range(nTest):\n",
    "    if len(numt[i])>=TW:\n",
    "        samplet.append(numt[i][-TW:,:])\n",
    "    else:\n",
    "        count_miss.append(i)\n",
    "samplet = np.array(samplet)\n",
    "\n",
    "labelt = [] \n",
    "for line in open(\"../CMAPSSData/RUL_FD00\"+Dataset+\".txt\"):\n",
    "    labelt.append(line.split())\n",
    "labelt = np.cast['int32'](labelt)\n",
    "labelnew = []\n",
    "for i in range(nTest):\n",
    "    if i not in count_miss:\n",
    "        #labelnew.append(labelt[i][0])\n",
    "        labelnew.append(labelt[i][0] if labelt[i][0]<=ConstRUL else ConstRUL)\n",
    "labelt = labelnew\n",
    "labelt=np.array(labelt)\n",
    "\n",
    "'''seed = 2222\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(samples)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(targets)\n",
    "samplet = samplet[np.argsort(labelt)]\n",
    "labelt = labelt[np.argsort(labelt)]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "(17731, 30, 14)\n",
      "(17731,)\n",
      "Testing data\n",
      "(100, 30, 14)\n",
      "(100,)\n",
      "Training data\n",
      "[[[ 0.42168675  0.12579028  0.17049291 ...  0.33333333 -0.27131783\n",
      "   -0.07953604]\n",
      "  [ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.27131783\n",
      "    0.01712234]\n",
      "  [-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  ...\n",
      "  [ 0.51204819  0.14453891  0.52464551 ...  0.         -0.62790698\n",
      "   -0.34217067]\n",
      "  [ 0.3253012   0.26444299  0.67623228 ...  0.         -1.\n",
      "   -0.17674676]\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]]\n",
      "\n",
      " [[ 0.06024096 -0.02419882  0.37812289 ...  0.16666667 -0.27131783\n",
      "    0.01712234]\n",
      "  [-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  [-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  ...\n",
      "  [ 0.3253012   0.26444299  0.67623228 ...  0.         -1.\n",
      "   -0.17674676]\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]]\n",
      "\n",
      " [[-0.01204819 -0.02419882  0.47467927 ...  0.16666667 -0.11627907\n",
      "   -0.11101906]\n",
      "  [-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  ...\n",
      "  [ 0.37349398  0.17462394  0.5658339  ...  0.5        -0.45736434\n",
      "   -0.78099972]\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]]\n",
      "\n",
      " [[-0.09638554 -0.02419882  0.53376097 ...  0.         -0.24031008\n",
      "   -0.2761668 ]\n",
      "  [ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  [ 0.06024096  0.36516242  0.4409183  ...  0.         -0.45736434\n",
      "   -0.17702292]\n",
      "  ...\n",
      "  [ 0.40361446  0.4589056   0.73295071 ...  0.16666667 -0.75193798\n",
      "   -0.26760563]\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]\n",
      "  [ 0.21686747  0.49204273  0.49493585 ...  0.16666667 -0.76744186\n",
      "   -0.53106877]]\n",
      "\n",
      " [[ 0.10843373 -0.01809462  0.29068197 ...  0.33333333  0.03875969\n",
      "   -0.28997514]\n",
      "  [ 0.06024096  0.36516242  0.4409183  ...  0.         -0.45736434\n",
      "   -0.17702292]\n",
      "  [ 0.31927711  0.06780031  0.22822417 ... -0.16666667 -0.56589147\n",
      "   -0.21264844]\n",
      "  ...\n",
      "  [ 0.3313253   0.36995858  0.55064146 ...  0.66666667 -0.53488372\n",
      "   -0.89201878]\n",
      "  [ 0.21686747  0.49204273  0.49493585 ...  0.16666667 -0.76744186\n",
      "   -0.53106877]\n",
      "  [ 0.59036145  0.2792675   0.68433491 ...  0.33333333 -0.64341085\n",
      "   -0.56365645]]]\n",
      "[4 3 2 1 0]\n",
      "Testing data\n",
      "[[[-0.55421687 -0.61107478 -0.55232951 ... -0.33333333  0.39534884\n",
      "    0.55426678]\n",
      "  [-0.65060241 -0.17680401 -0.75151924 ... -0.33333333  0.28682171\n",
      "    0.14526374]\n",
      "  [-0.55421687 -0.58098975 -0.41458474 ... -0.33333333  0.41085271\n",
      "    0.341066  ]\n",
      "  ...\n",
      "  [-0.46987952  0.10660562 -0.28257934 ... -0.16666667  0.34883721\n",
      "    0.38663353]\n",
      "  [-0.24698795 -0.3381295  -0.24004051 ... -0.16666667  0.51937984\n",
      "    0.11875173]\n",
      "  [-0.34337349 -0.13494659 -0.47029034 ... -0.5         0.27131783\n",
      "    0.56420878]]\n",
      "\n",
      " [[-0.02409639 -0.30978853 -0.32343011 ... -0.33333333  0.37984496\n",
      "    0.25158796]\n",
      "  [-0.1626506  -0.29016787 -0.23869007 ... -0.16666667  0.27131783\n",
      "    0.31593482]\n",
      "  [-0.39156627 -0.29627207 -0.36664416 ... -0.16666667  0.25581395\n",
      "    0.3985087 ]\n",
      "  ...\n",
      "  [-0.40361446 -0.21342926 -0.32039163 ... -0.16666667  0.37984496\n",
      "    0.31759183]\n",
      "  [-0.39156627 -0.39481142 -0.26772451 ... -0.33333333  0.06976744\n",
      "    0.50483292]\n",
      "  [-0.1686747  -0.48027033 -0.03207292 ...  0.16666667 -0.27131783\n",
      "    0.10770505]]\n",
      "\n",
      " [[-0.15662651 -0.05777196 -0.29642134 ... -0.16666667  0.17829457\n",
      "    0.11543772]\n",
      "  [-0.24698795 -0.23348594 -0.11006077 ... -0.16666667  0.1627907\n",
      "    0.1248274 ]\n",
      "  [-0.18072289 -0.24177022 -0.04051317 ... -0.33333333  0.2248062\n",
      "    0.34962717]\n",
      "  ...\n",
      "  [ 0.12048193 -0.27883148  0.05773126 ...  0.16666667 -0.10077519\n",
      "    0.02734051]\n",
      "  [ 0.01204819 -0.03727927 -0.28899392 ...  0.         -0.02325581\n",
      "    0.00303783]\n",
      "  [-0.11445783  0.24133421  0.1215395  ...  0.         -0.03875969\n",
      "    0.28859431]]\n",
      "\n",
      " [[-0.40963855 -0.19032047 -0.56617151 ... -0.66666667  0.42635659\n",
      "    0.32836233]\n",
      "  [-0.62048193 -0.57096141 -0.67555706 ... -0.66666667  0.51937984\n",
      "    0.4084507 ]\n",
      "  [-0.42771084 -0.36908655 -0.486158   ... -0.5         0.95348837\n",
      "    0.48715824]\n",
      "  ...\n",
      "  [-0.04216867 -0.5382603  -0.49729912 ... -0.5         0.44186047\n",
      "    0.08091687]\n",
      "  [-0.3253012  -0.46152169 -0.28865631 ... -0.66666667  0.28682171\n",
      "    0.54045844]\n",
      "  [-0.52409639 -0.39001526 -0.46893991 ... -0.5         0.25581395\n",
      "    0.28500414]]\n",
      "\n",
      " [[-0.30722892 -0.11881404 -0.1144497  ... -0.33333333  0.06976744\n",
      "    0.1996686 ]\n",
      "  [-0.10240964 -0.17680401 -0.13301823 ... -0.16666667  0.14728682\n",
      "    0.11516156]\n",
      "  [-0.02409639 -0.31981687 -0.02295746 ... -0.33333333  0.08527132\n",
      "   -0.12703673]\n",
      "  ...\n",
      "  [ 0.34337349 -0.03597122 -0.17049291 ...  0.16666667 -0.25581395\n",
      "   -0.1413974 ]\n",
      "  [ 0.23493976  0.0442555   0.25286968 ...  0.16666667 -0.19379845\n",
      "    0.03755869]\n",
      "  [ 0.04819277  0.33333333  0.44294396 ...  0.33333333 -0.13178295\n",
      "   -0.1955261 ]]]\n",
      "[125  82  59 117  20]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data\")\n",
    "print(samples.shape)\n",
    "print(targets.shape)\n",
    "print(\"Testing data\")\n",
    "print(samplet.shape)\n",
    "print(labelt.shape)\n",
    "print(\"Training data\")\n",
    "print(samples[-5:,:])\n",
    "print(targets[-5:])\n",
    "print(\"Testing data\")\n",
    "print(samplet[-5:,:])\n",
    "print(labelt[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n",
    "\n",
    "CNN model. The model is Dense(ReLU, 100)->Dense(ReLu, 100)->Dense(Linear, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RULCNNModel(TW, FeatureN):\n",
    "    \n",
    "    input_layer = Input(shape=(TW, FeatureN))\n",
    "    y = Reshape((TW, FeatureN, 1), input_shape=(TW, FeatureN, ),name = 'Reshape')(input_layer)\n",
    "\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C1')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C2')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C3')(y)\n",
    "    y = Conv2D(FilterN, FilterL, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='C4')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C5')(y)\n",
    "    #y = Convolution2D(FilterN, FilterL, 1, border_mode='same', init='glorot_normal', activation='tanh', name='C6')(y)\n",
    "    \n",
    "    y = Conv2D(1, 3, 1, border_mode='same', kernel_initializer='glorot_normal', activation='tanh', name='Clast')(y)  \n",
    "    \n",
    "    y = Reshape((TW,14))(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    \n",
    "    #y = Dense(100, activation='tanh', init='glorot_normal', activity_regularizer=keras.regularizers.l2(0.01),)(y)\n",
    "    y = Dense(100,activation='tanh', kernel_initializer='glorot_normal', name='fc')(y)\n",
    "    y = Dense(1)(y)\n",
    "    \n",
    "    model = Model(inputs = input_layer, outputs = y, name='RUL_CNN_Model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the keras model\n",
    "\n",
    "Fit the Keras model to the data and determine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), name=\"C1\", activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), name=\"C2\", activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\")`\n",
      "  import sys\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), name=\"C3\", activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (10, 1), name=\"C4\", activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(1, (3, 1), name=\"Clast\", activation=\"tanh\", kernel_initializer=\"glorot_normal\", padding=\"same\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\controlslab\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17731 samples, validate on 100 samples\n",
      "Epoch 1/250\n",
      "17731/17731 [==============================] - 1s 80us/step - loss: 6477.6097 - val_loss: 4985.7393\n",
      "Epoch 2/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 5653.3891 - val_loss: 4567.2373\n",
      "Epoch 3/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 5211.3580 - val_loss: 4197.2041\n",
      "Epoch 4/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4812.7686 - val_loss: 3858.3606\n",
      "Epoch 5/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4446.7817 - val_loss: 3547.8027\n",
      "Epoch 6/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 4109.5075 - val_loss: 3260.2002\n",
      "Epoch 7/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3797.7512 - val_loss: 2996.3311\n",
      "Epoch 8/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3509.0816 - val_loss: 2751.6165\n",
      "Epoch 9/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 3240.2442 - val_loss: 2523.8752\n",
      "Epoch 10/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 2992.2093 - val_loss: 2314.2031\n",
      "Epoch 11/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2761.1742 - val_loss: 2102.3293\n",
      "Epoch 12/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2550.9054 - val_loss: 1928.0995\n",
      "Epoch 13/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2375.5849 - val_loss: 1806.9161\n",
      "Epoch 14/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2179.7666 - val_loss: 1615.4738\n",
      "Epoch 15/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 2008.0248 - val_loss: 1466.1086\n",
      "Epoch 16/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1854.8524 - val_loss: 1353.0422\n",
      "Epoch 17/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1714.1985 - val_loss: 1289.3364\n",
      "Epoch 18/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1585.3830 - val_loss: 1168.4893\n",
      "Epoch 19/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1470.4637 - val_loss: 1032.8533\n",
      "Epoch 20/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1362.7216 - val_loss: 941.4451\n",
      "Epoch 21/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1254.0067 - val_loss: 854.3477\n",
      "Epoch 22/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1169.6519 - val_loss: 782.8167\n",
      "Epoch 23/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1081.3043 - val_loss: 729.3741\n",
      "Epoch 24/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 1003.2748 - val_loss: 661.1526\n",
      "Epoch 25/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 954.5351 - val_loss: 609.1388\n",
      "Epoch 26/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 870.7291 - val_loss: 624.3723\n",
      "Epoch 27/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 813.7911 - val_loss: 551.2740\n",
      "Epoch 28/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 759.5973 - val_loss: 474.1255\n",
      "Epoch 29/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 707.9052 - val_loss: 458.0619\n",
      "Epoch 30/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 669.5223 - val_loss: 415.3335\n",
      "Epoch 31/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 645.3259 - val_loss: 390.6812\n",
      "Epoch 32/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 580.1220 - val_loss: 366.4863\n",
      "Epoch 33/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 574.8853 - val_loss: 346.5088\n",
      "Epoch 34/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 531.5406 - val_loss: 311.0401\n",
      "Epoch 35/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 500.8808 - val_loss: 364.2889\n",
      "Epoch 36/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 472.9040 - val_loss: 284.1460\n",
      "Epoch 37/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 456.5299 - val_loss: 280.3760\n",
      "Epoch 38/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 418.0784 - val_loss: 247.1887\n",
      "Epoch 39/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 425.9594 - val_loss: 255.7038\n",
      "Epoch 40/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 385.5517 - val_loss: 281.0505\n",
      "Epoch 41/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 400.4078 - val_loss: 237.9379\n",
      "Epoch 42/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 354.3338 - val_loss: 225.9738\n",
      "Epoch 43/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 344.5488 - val_loss: 223.1089\n",
      "Epoch 44/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 345.2718 - val_loss: 211.2813\n",
      "Epoch 45/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 323.2973 - val_loss: 206.6478\n",
      "Epoch 46/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 300.4731 - val_loss: 243.1761\n",
      "Epoch 47/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 347.6192 - val_loss: 178.3787\n",
      "Epoch 48/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 288.7276 - val_loss: 217.9174\n",
      "Epoch 49/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 280.8605 - val_loss: 202.5936\n",
      "Epoch 50/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 278.5621 - val_loss: 169.9358\n",
      "Epoch 51/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 278.6901 - val_loss: 168.4143\n",
      "Epoch 52/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 270.5632 - val_loss: 162.8846\n",
      "Epoch 53/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 265.9137 - val_loss: 168.3025\n",
      "Epoch 54/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 247.5826 - val_loss: 170.9837\n",
      "Epoch 55/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 245.7759 - val_loss: 233.9788\n",
      "Epoch 56/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 255.5840 - val_loss: 171.5747\n",
      "Epoch 57/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 244.8946 - val_loss: 162.6182\n",
      "Epoch 58/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 231.2859 - val_loss: 233.9455\n",
      "Epoch 59/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 232.7845 - val_loss: 227.8277\n",
      "Epoch 60/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 218.5140 - val_loss: 180.4481\n",
      "Epoch 61/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 232.6600 - val_loss: 206.1443\n",
      "Epoch 62/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 223.8402 - val_loss: 228.0218\n",
      "Epoch 63/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 212.1247 - val_loss: 178.2937\n",
      "Epoch 64/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 249.7101 - val_loss: 173.8098\n",
      "Epoch 65/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 201.4783 - val_loss: 155.2798\n",
      "Epoch 66/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 207.2830 - val_loss: 169.9095\n",
      "Epoch 67/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 212.8323 - val_loss: 155.7529\n",
      "Epoch 68/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 205.8497 - val_loss: 185.2046\n",
      "Epoch 69/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 193.5995 - val_loss: 182.1910\n",
      "Epoch 70/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 201.7511 - val_loss: 160.8312\n",
      "Epoch 71/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 201.9154 - val_loss: 155.5052\n",
      "Epoch 72/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 199.0227 - val_loss: 156.4651\n",
      "Epoch 73/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 191.1263 - val_loss: 172.1963\n",
      "Epoch 74/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - 0s 23us/step - loss: 188.0168 - val_loss: 179.9889\n",
      "Epoch 75/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 209.8040 - val_loss: 170.7365\n",
      "Epoch 76/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 181.0095 - val_loss: 160.1453\n",
      "Epoch 77/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 187.8749 - val_loss: 151.4225\n",
      "Epoch 78/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 177.4997 - val_loss: 215.1797\n",
      "Epoch 79/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 187.7312 - val_loss: 154.0558\n",
      "Epoch 80/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 176.9077 - val_loss: 171.2271\n",
      "Epoch 81/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 191.0667 - val_loss: 376.8734\n",
      "Epoch 82/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 202.7675 - val_loss: 169.1406\n",
      "Epoch 83/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.6638 - val_loss: 196.0511\n",
      "Epoch 84/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 186.8502 - val_loss: 163.6606\n",
      "Epoch 85/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.5006 - val_loss: 181.8582\n",
      "Epoch 86/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.9328 - val_loss: 151.3548\n",
      "Epoch 87/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 192.1097 - val_loss: 157.3137\n",
      "Epoch 88/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.8164 - val_loss: 151.2054\n",
      "Epoch 89/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.3630 - val_loss: 230.5766\n",
      "Epoch 90/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.6603 - val_loss: 158.9491\n",
      "Epoch 91/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 175.9065 - val_loss: 154.1750\n",
      "Epoch 92/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.1256 - val_loss: 147.6470\n",
      "Epoch 93/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 172.6975 - val_loss: 210.1681\n",
      "Epoch 94/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.6125 - val_loss: 154.7840\n",
      "Epoch 95/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.4137 - val_loss: 163.7277\n",
      "Epoch 96/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 161.3055 - val_loss: 154.6709\n",
      "Epoch 97/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 181.5431 - val_loss: 149.8841\n",
      "Epoch 98/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 163.5261 - val_loss: 200.3883\n",
      "Epoch 99/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 168.6860 - val_loss: 241.0202\n",
      "Epoch 100/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 159.7752 - val_loss: 172.2439\n",
      "Epoch 101/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 164.5847 - val_loss: 158.7271\n",
      "Epoch 102/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.1115 - val_loss: 185.4342\n",
      "Epoch 103/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 173.4775 - val_loss: 161.5564\n",
      "Epoch 104/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.2817 - val_loss: 150.4652\n",
      "Epoch 105/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 151.0763 - val_loss: 148.4025\n",
      "Epoch 106/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 184.7690 - val_loss: 168.0824\n",
      "Epoch 107/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.0374 - val_loss: 153.8389\n",
      "Epoch 108/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 151.9038 - val_loss: 194.3887\n",
      "Epoch 109/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.0219 - val_loss: 144.7539\n",
      "Epoch 110/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.2161 - val_loss: 152.3292\n",
      "Epoch 111/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 155.4696 - val_loss: 142.6570\n",
      "Epoch 112/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 156.3253 - val_loss: 241.0485\n",
      "Epoch 113/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.2247 - val_loss: 175.0014\n",
      "Epoch 114/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 153.4815 - val_loss: 165.0634\n",
      "Epoch 115/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 147.5281 - val_loss: 142.2181\n",
      "Epoch 116/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 161.8966 - val_loss: 164.0906\n",
      "Epoch 117/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.2320 - val_loss: 149.2955\n",
      "Epoch 118/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 160.0370 - val_loss: 302.9924\n",
      "Epoch 119/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 162.5918 - val_loss: 148.1570\n",
      "Epoch 120/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.7508 - val_loss: 151.6578\n",
      "Epoch 121/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 146.3260 - val_loss: 147.3568\n",
      "Epoch 122/250\n",
      "17731/17731 [==============================] - 0s 21us/step - loss: 160.4296 - val_loss: 156.6291\n",
      "Epoch 123/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.4792 - val_loss: 175.0441\n",
      "Epoch 124/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 170.4645 - val_loss: 139.7286\n",
      "Epoch 125/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.6399 - val_loss: 271.4661\n",
      "Epoch 126/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.2134 - val_loss: 151.7586\n",
      "Epoch 127/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.9456 - val_loss: 204.8175\n",
      "Epoch 128/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 145.0736 - val_loss: 144.8055\n",
      "Epoch 129/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 160.0890 - val_loss: 152.0967\n",
      "Epoch 130/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.0090 - val_loss: 151.3878\n",
      "Epoch 131/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 150.9140 - val_loss: 160.2576\n",
      "Epoch 132/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.0728 - val_loss: 137.1801\n",
      "Epoch 133/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 150.7884 - val_loss: 165.2848\n",
      "Epoch 134/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 140.5935 - val_loss: 141.4145\n",
      "Epoch 135/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.5748 - val_loss: 144.4645\n",
      "Epoch 136/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 149.5479 - val_loss: 141.3232\n",
      "Epoch 137/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 147.9622 - val_loss: 168.2923\n",
      "Epoch 138/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.4442 - val_loss: 182.4848\n",
      "Epoch 139/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 157.9378 - val_loss: 146.7931\n",
      "Epoch 140/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.0151 - val_loss: 141.3818\n",
      "Epoch 141/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.9704 - val_loss: 205.0077\n",
      "Epoch 142/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.6165 - val_loss: 164.2195\n",
      "Epoch 143/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.5514 - val_loss: 153.2810\n",
      "Epoch 144/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 140.5600 - val_loss: 166.5678\n",
      "Epoch 145/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.7215 - val_loss: 213.7335\n",
      "Epoch 146/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.7754 - val_loss: 255.0202\n",
      "Epoch 147/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 159.3122 - val_loss: 162.8225\n",
      "Epoch 148/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.8035 - val_loss: 200.3675\n",
      "Epoch 149/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.9030 - val_loss: 180.9111\n",
      "Epoch 150/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.7830 - val_loss: 194.6504\n",
      "Epoch 151/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.5664 - val_loss: 151.4472\n",
      "Epoch 152/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.3035 - val_loss: 172.3901\n",
      "Epoch 153/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 154.5108 - val_loss: 142.4039\n",
      "Epoch 154/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 144.9730 - val_loss: 157.8159\n",
      "Epoch 155/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.2303 - val_loss: 146.9393\n",
      "Epoch 156/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.9762 - val_loss: 143.1102\n",
      "Epoch 157/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 136.6394 - val_loss: 162.4358\n",
      "Epoch 158/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 142.9164 - val_loss: 163.4390\n",
      "Epoch 159/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.5335 - val_loss: 152.1265\n",
      "Epoch 160/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 139.5665 - val_loss: 147.0853\n",
      "Epoch 161/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 136.0051 - val_loss: 142.7111\n",
      "Epoch 162/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 148.0132 - val_loss: 145.0917\n",
      "Epoch 163/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.0928 - val_loss: 180.0760\n",
      "Epoch 164/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.4012 - val_loss: 208.1665\n",
      "Epoch 165/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.2430 - val_loss: 141.3799\n",
      "Epoch 166/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.9326 - val_loss: 186.0801\n",
      "Epoch 167/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.4220 - val_loss: 142.2436\n",
      "Epoch 168/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.4835 - val_loss: 148.9898\n",
      "Epoch 169/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.2586 - val_loss: 152.0912\n",
      "Epoch 170/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 138.8140 - val_loss: 151.8699\n",
      "Epoch 171/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.6491 - val_loss: 154.6669\n",
      "Epoch 172/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.7087 - val_loss: 154.1929\n",
      "Epoch 173/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 141.0237 - val_loss: 170.4592\n",
      "Epoch 174/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 134.6455 - val_loss: 143.6577\n",
      "Epoch 175/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 133.8255 - val_loss: 174.0845\n",
      "Epoch 176/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 140.4043 - val_loss: 164.5400\n",
      "Epoch 177/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.7432 - val_loss: 264.6820\n",
      "Epoch 178/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.3867 - val_loss: 165.7436\n",
      "Epoch 179/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.8970 - val_loss: 160.6441\n",
      "Epoch 180/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.2987 - val_loss: 151.4475\n",
      "Epoch 181/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.4332 - val_loss: 144.1405\n",
      "Epoch 182/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 128.0294 - val_loss: 149.9190\n",
      "Epoch 183/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 133.2244 - val_loss: 159.5775\n",
      "Epoch 184/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 143.4000 - val_loss: 146.3401\n",
      "Epoch 185/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 129.8549 - val_loss: 147.6977\n",
      "Epoch 186/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 174.8750 - val_loss: 167.3334\n",
      "Epoch 187/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.7780 - val_loss: 194.2078\n",
      "Epoch 188/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.6182 - val_loss: 152.2422\n",
      "Epoch 189/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 130.1944 - val_loss: 165.3491\n",
      "Epoch 190/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 135.1053 - val_loss: 185.6008\n",
      "Epoch 191/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 132.0984 - val_loss: 182.8444\n",
      "Epoch 192/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 128.1237 - val_loss: 182.8753\n",
      "Epoch 193/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 127.9118 - val_loss: 169.0776\n",
      "Epoch 194/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 127.7178 - val_loss: 155.3567\n",
      "Epoch 195/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 140.8786 - val_loss: 152.2632\n",
      "Epoch 196/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 125.9807 - val_loss: 169.9428\n",
      "Epoch 197/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.9158 - val_loss: 145.7686\n",
      "Epoch 198/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 152.1076 - val_loss: 251.8048\n",
      "Epoch 199/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 137.7720 - val_loss: 154.1607\n",
      "Epoch 200/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 131.9940 - val_loss: 158.1200\n",
      "Epoch 201/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.0495 - val_loss: 152.1704\n",
      "Epoch 202/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.4395 - val_loss: 155.7327\n",
      "Epoch 203/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 122.2777 - val_loss: 155.6026\n",
      "Epoch 204/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.7680 - val_loss: 164.7876\n",
      "Epoch 205/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.5478 - val_loss: 154.5833\n",
      "Epoch 206/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.8731 - val_loss: 153.8526\n",
      "Epoch 207/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.6135 - val_loss: 160.7575\n",
      "Epoch 208/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 120.6024 - val_loss: 160.2004\n",
      "Epoch 209/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.8658 - val_loss: 152.6035\n",
      "Epoch 210/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.1148 - val_loss: 156.3642\n",
      "Epoch 211/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.2900 - val_loss: 157.9801\n",
      "Epoch 212/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.3288 - val_loss: 151.4859\n",
      "Epoch 213/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.7497 - val_loss: 159.8156\n",
      "Epoch 214/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.6214 - val_loss: 156.2955\n",
      "Epoch 215/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.4986 - val_loss: 157.0722\n",
      "Epoch 216/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.6705 - val_loss: 153.9055\n",
      "Epoch 217/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.7737 - val_loss: 158.0384\n",
      "Epoch 218/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 121.1041 - val_loss: 153.7775\n",
      "Epoch 219/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.1384 - val_loss: 158.5872\n",
      "Epoch 220/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17731/17731 [==============================] - ETA: 0s - loss: 120.061 - 0s 22us/step - loss: 119.7644 - val_loss: 154.6385\n",
      "Epoch 221/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.9979 - val_loss: 156.2561\n",
      "Epoch 222/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 120.9058 - val_loss: 154.4792\n",
      "Epoch 223/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.7322 - val_loss: 154.4244\n",
      "Epoch 224/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.9382 - val_loss: 160.5224\n",
      "Epoch 225/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.2956 - val_loss: 154.7111\n",
      "Epoch 226/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.4925 - val_loss: 158.9643\n",
      "Epoch 227/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.3448 - val_loss: 159.0265\n",
      "Epoch 228/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.7513 - val_loss: 160.1031\n",
      "Epoch 229/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.8966 - val_loss: 159.7722\n",
      "Epoch 230/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.2694 - val_loss: 154.3776\n",
      "Epoch 231/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.2663 - val_loss: 159.4027\n",
      "Epoch 232/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.9857 - val_loss: 165.3991\n",
      "Epoch 233/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.2246 - val_loss: 155.6501\n",
      "Epoch 234/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 121.1953 - val_loss: 159.7786\n",
      "Epoch 235/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.2242 - val_loss: 163.3214\n",
      "Epoch 236/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.9113 - val_loss: 156.6667\n",
      "Epoch 237/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.4007 - val_loss: 160.0365\n",
      "Epoch 238/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.4231 - val_loss: 152.6352\n",
      "Epoch 239/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.1446 - val_loss: 161.5796\n",
      "Epoch 240/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 120.2920 - val_loss: 154.2968\n",
      "Epoch 241/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.7752 - val_loss: 159.3222\n",
      "Epoch 242/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.1152 - val_loss: 159.9445\n",
      "Epoch 243/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 120.1106 - val_loss: 157.2893\n",
      "Epoch 244/250\n",
      "17731/17731 [==============================] - 0s 23us/step - loss: 119.0893 - val_loss: 155.2341\n",
      "Epoch 245/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.4794 - val_loss: 158.4211\n",
      "Epoch 246/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.6043 - val_loss: 159.7947\n",
      "Epoch 247/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.6267 - val_loss: 154.4357\n",
      "Epoch 248/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 117.5063 - val_loss: 159.4249\n",
      "Epoch 249/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 119.4984 - val_loss: 162.0734\n",
      "Epoch 250/250\n",
      "17731/17731 [==============================] - 0s 22us/step - loss: 118.0415 - val_loss: 157.8257\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0, beta_1=0.5)\n",
    "#DCNN = Model([input_layer], [y])\n",
    "DCNN = RULCNNModel(TW, FeatureN)\n",
    "#DCNN.compile(loss=get_score,optimizer=opt)\n",
    "DCNN.compile(loss='mean_squared_error',optimizer=opt)\n",
    "lrate = LearningRateScheduler(CMAPSAuxFunctions.step_decay)\n",
    "\n",
    "\n",
    "startTime = time.clock()\n",
    "history = DCNN.fit(samples, targets,nb_epoch=nb_epoch, batch_size=batch_size,verbose=1, \n",
    "                   validation_data=(samplet, labelt), callbacks=[lrate])\n",
    "endTime = time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 80us/step\n",
      "Root Square Mean Error score: 12.56286816109703\n",
      "Health score: [270.5371]\n",
      "Elapsed time: 99.38800193682309\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "score = DCNN.evaluate(samplet, labelt)\n",
    "y_pred = DCNN.predict(samplet)\n",
    "healtScore = CMAPSAuxFunctions.compute_health_score(labelt, y_pred)\n",
    "\n",
    "print(\"Root Square Mean Error score: {}\".format(np.sqrt(score)))\n",
    "print(\"Health score: {}\".format(healtScore))\n",
    "print(\"Elapsed time: {}\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
