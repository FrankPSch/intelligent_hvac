{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Test notebook for the C-MAPPS benchmark. Approach using MLP. \n",
    "\n",
    "First we import the necessary packages and create the global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "#import plottingTools\n",
    "#from datetime import datetime\n",
    "#from sklearn.covariance import EllipticEnvelope\n",
    "#from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.dummy import DummyClassifier\n",
    "#from sklearn.model_selection import train_test_split, cross_validate\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#from dataManagement import DataManagerDamadics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib notebook\n",
    "\n",
    "global constRUL\n",
    "\n",
    "constRUL = 125\n",
    "time_window = 30\n",
    "rul_vector = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_training_RUL(df_row, *args):\n",
    "    \n",
    "    global constRUL\n",
    "    rul_vector = args[0]\n",
    "    \n",
    "    if rul_vector[int(df_row['Unit Number']) - 1] - df_row['Cycle'] > constRUL:\n",
    "        return constRUL\n",
    "    else:\n",
    "        return rul_vector[int(df_row['Unit Number']) - 1] - df_row['Cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_from_df(df, time_window, features, num_units, dataset_type):\n",
    "    \n",
    "    n_m = df.shape[0]\n",
    "    n_x = len(features)\n",
    "    \n",
    "    df_values = df[features].values\n",
    "    targets = df['RUL'].values\n",
    "    n_m = 0\n",
    "    n_X = len(features)\n",
    "    df_unit_values = []\n",
    "    targets_unit = []\n",
    "    num_samples_unit = []\n",
    "    \n",
    "    #Count number of elements at each group so that we can create the matrix to hold them all. \n",
    "    #Also store each matrix in temporary arrays to access them faster\n",
    "    for i in range(1,num_units+1):\n",
    "        \n",
    "        df_unit = df.loc[df['Unit Number'] == i]\n",
    "        df_unit_values.append(df_unit[features].values) #is this a view or a copy of the df?\n",
    "        targets_unit.append(df_unit['RUL'].values) #is this a view or a copy of the df?\n",
    "        num_samples_unit.append(df_unit.shape[0])\n",
    "        n_m = n_m + num_samples_unit[i-1]-time_window+1\n",
    "    \n",
    "    #Create the numpy arrays to hold the features\n",
    "    if (dataset_type == 'train' or dataset_type == 'cross_validation'):\n",
    "        X, y = np.empty([n_m, n_x*time_window]), np.empty([n_m, 1])\n",
    "    else:\n",
    "        X, y = np.empty([num_units, n_x*time_window]), np.empty([num_units, 1])\n",
    "        \n",
    "    k = 0\n",
    "    \n",
    "    #Create the feature matrix by moving the time window for each type of engine.\n",
    "    for i in range(num_units):\n",
    "    \n",
    "        if (dataset_type == 'train' or dataset_type == 'cross_validation'):\n",
    "            for j in range(num_samples_unit[i]-time_window+1):\n",
    "\n",
    "                time_window_samples = df_unit_values[i][j:j+time_window]\n",
    "                X[k,:] = np.squeeze(time_window_samples.reshape(1,-1))\n",
    "                y[k] = targets_unit[i][j+time_window-1]\n",
    "                k = k + 1\n",
    "        else:\n",
    "            #print(dataset_type)\n",
    "            time_window_samples = df_unit_values[i][-time_window:]\n",
    "            X[k,:] = np.squeeze(time_window_samples.reshape(1,-1))\n",
    "            k = k + 1\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and Reshape data\n",
    "\n",
    "Get the data from the text files, store it in a Pandas Dataframe and reshape it as appropiately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_and_reshape_data(from_file, columns, time_window, dataset_type):\n",
    "    '''\n",
    "    T2        - Total temperature at fan inlet      R\n",
    "    T24       - Total temperature at lpc outlet     R\n",
    "    T30       - Total temperature at hpc outlet     R\n",
    "    T50       - Total temperature at LPT outlet     R\n",
    "    P2        - Pressure at fan inlet               psia\n",
    "    P15       - Total pressure in bypass-duct       psia\n",
    "    P30       - Total pressure at HPC outlet        psia\n",
    "    Nf        - Physical fan speed                  rpm\n",
    "    Nc        - Physical core speed                 rpm\n",
    "    epr       - Engine Pressure ratio (P50/P2)      --\n",
    "    Ps30      - Static Pressure at HPC outlet       psia\n",
    "    phi       - Ratio fuel flow to Ps30             pps/psi\n",
    "    NRf       - corrected fan speed                 rpm\n",
    "    NRc       - Corrected core speed                rpm\n",
    "    BPR       - Bypass ratio                        --\n",
    "    farB      - Burner fuel-air ratio               --\n",
    "    htBleed   - Bleed enthalpy                      --\n",
    "    Nf_dmd    - Demanded fan speed                  rpm\n",
    "    PCNfR_dmd - Demanded corrected fan speed        rpm\n",
    "    W31       - HPT coolant bleed                   lbm/s\n",
    "    W32       - LPT coolant bleed                   lbm/s\n",
    "    '''\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(from_file ,sep='\\s+',header=None)\n",
    "\n",
    "    col_names = {0:'Unit Number', 1:'Cycle', 2:'Op. Settings 1', 3:'Op. Settings 2', 4:'Op. Settings 3', 5:'T2',\n",
    "                6:'T24', 7:'T30', 8:'T50', 9:'P2', 10:'P15', 11:'P30', 12:'Nf', 13:'Nc', 14:'epr', 15:'Ps30', \n",
    "                16:'phi', 17:'NRf', 18:'NRc', 19:'BPR', 20:'farB', 21:'htBleed', 22:'Nf_dmd', 23:'PCNfR_dmd', \n",
    "                24:'W31', 25:'W32'}\n",
    "\n",
    "    df.rename(columns=col_names, inplace=True)\n",
    "\n",
    "    gruoped_by_unit = df.groupby('Unit Number')\n",
    "    rul_vector = gruoped_by_unit.size().values\n",
    "    num_units = len(gruoped_by_unit)\n",
    "\n",
    "    df['RUL'] = df.apply(compute_training_RUL, axis = 1, args=(rul_vector,))\n",
    "    selected_features_rul = columns[:]\n",
    "    selected_features_rul.extend(['Unit Number', 'RUL'])\n",
    "    df_selected_features = df[selected_features_rul]\n",
    "\n",
    "    X, y = get_X_y_from_df(df_selected_features, time_window, selected_features, num_units, dataset_type)\n",
    "    \n",
    "    #display(df_selected_features.head(5))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "    '''display(df_selected_features)\n",
    "    print(X.shape)\n",
    "    print(X)\n",
    "    print(y.shape)\n",
    "    print(y)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras model\n",
    "\n",
    "We will use a very simple ANN for this example. The model is Dense(ReLU, 100)->Dense(ReLu, 100)->Dense(Linear, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RULmodel(input_shape):\n",
    "    \n",
    "    print(input_shape)\n",
    "    \n",
    "    #Create a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Add the layers for the model\n",
    "    model.add(Dense(1000, input_dim=input_shape, activation='relu', name='fc1'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(500, activation='relu', name='fc2'))\n",
    "    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(100, activation='relu', name='fc3'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(10, activation='relu', name='fc4'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='linear', name='out'))\n",
    "    \n",
    "    #create a placeholder for the input\n",
    "    #X_input = Input(shape=(input_shape))\n",
    "    \n",
    "    #Create the layers\n",
    "    #X = Dense(100, activation='relu', name='fc1')(X_input)\n",
    "    #X = Dense(100, activation='relu', name='fc2')(X)\n",
    "    #X = Dense(1, activation='linear', name='out')(X)\n",
    "    \n",
    "    # Create model. This creates the Keras model instance, you'll use this instance to train/test the model.\n",
    "    #model = Sequential(inputs = X_input, outputs = X, name='RUL')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the train and test data from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_train = '../CMAPSSData/train_FD001.txt'\n",
    "data_file_test = '../CMAPSSData/test_FD001.txt'\n",
    "\n",
    "#min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "standardScaler = StandardScaler()\n",
    "\n",
    "#Selected as per CNN paper\n",
    "selected_features = ['T2', 'T24', 'T30', 'P15', 'P30', 'Nf', 'epr', 'Ps30', 'phi', 'NRf', 'NRc', 'farB', 'PCNfR_dmd', 'W31']\n",
    "\n",
    "#Get the X and y matrices with the specified time window\n",
    "X_train, y_train = retrieve_and_reshape_data(data_file_train, selected_features, time_window, 'train')\n",
    "X_test, _ = retrieve_and_reshape_data(data_file_test, selected_features, time_window, 'test')\n",
    "y_test = np.loadtxt(\"../CMAPSSData/RUL_FD001.txt\")\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "\n",
    "#Standardize the data\n",
    "X_train = standardScaler.fit_transform(X_train)\n",
    "X_test = standardScaler.fit_transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17731, 420)\n",
      "(17731, 1)\n",
      "(100, 420)\n",
      "(100, 1)\n",
      "[[ 1.          2.41264288  1.49234946 ...,  1.          0.         -1.69899007]\n",
      " [ 1.          0.97293491  0.82670323 ...,  1.          0.         -2.7492681 ]\n",
      " [ 1.          0.68499331  0.82670323 ...,  1.          0.         -1.97537902]\n",
      " [ 1.          0.34906145  0.82670323 ...,  1.          0.         -2.80454589]\n",
      " [ 1.          1.16489597  0.85379348 ...,  1.          0.         -2.36232356]]\n",
      "[[ 4.]\n",
      " [ 3.]\n",
      " [ 2.]\n",
      " [ 1.]\n",
      " [ 0.]]\n",
      "[[ 1.         -1.54886617 -1.88125364 ...,  1.          0.          1.25789073]\n",
      " [ 1.          0.59455611 -0.54071589 ...,  1.          0.         -0.99754859]\n",
      " [ 1.          0.05870054  0.58060223 ...,  1.          0.         -0.03093174]\n",
      " [ 1.         -0.96429646 -0.00915678 ...,  1.          0.          1.1934496 ]\n",
      " [ 1.         -0.55022624  0.30900268 ...,  1.          0.         -0.41757848]]\n",
      "[[ 137.]\n",
      " [  82.]\n",
      " [  59.]\n",
      " [ 117.]\n",
      " [  20.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train[-5:,:])\n",
    "print(y_train[-5:,:])\n",
    "print(X_test[-5:,:])\n",
    "print(y_test[-5:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the keras model\n",
    "\n",
    "Fit the Keras model to the data and determine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n",
      "Epoch 1/200\n",
      "17731/17731 [==============================] - 5s 301us/step - loss: 719.8961 - acc: 0.0235\n",
      "Epoch 2/200\n",
      "17731/17731 [==============================] - 5s 277us/step - loss: 281.2681 - acc: 0.0324\n",
      "Epoch 3/200\n",
      "17731/17731 [==============================] - 5s 277us/step - loss: 266.1248 - acc: 0.0327 0s - loss: 265.8550 \n",
      "Epoch 4/200\n",
      "17731/17731 [==============================] - 5s 284us/step - loss: 265.3448 - acc: 0.0339\n",
      "Epoch 5/200\n",
      "17731/17731 [==============================] - 5s 274us/step - loss: 263.0590 - acc: 0.0350\n",
      "Epoch 6/200\n",
      "17731/17731 [==============================] - 5s 281us/step - loss: 261.5475 - acc: 0.0343\n",
      "Epoch 7/200\n",
      "17731/17731 [==============================] - 5s 279us/step - loss: 251.5504 - acc: 0.0322\n",
      "Epoch 8/200\n",
      "17731/17731 [==============================] - 5s 301us/step - loss: 240.4054 - acc: 0.0345\n",
      "Epoch 9/200\n",
      "17731/17731 [==============================] - 5s 278us/step - loss: 237.1390 - acc: 0.0370\n",
      "Epoch 10/200\n",
      "17731/17731 [==============================] - 5s 281us/step - loss: 240.1793 - acc: 0.0356\n",
      "Epoch 11/200\n",
      "17731/17731 [==============================] - 6s 314us/step - loss: 241.9858 - acc: 0.0371\n",
      "Epoch 12/200\n",
      "17731/17731 [==============================] - 5s 280us/step - loss: 234.8348 - acc: 0.0390\n",
      "Epoch 13/200\n",
      "17731/17731 [==============================] - 5s 281us/step - loss: 232.5571 - acc: 0.0357\n",
      "Epoch 14/200\n",
      "17731/17731 [==============================] - 5s 285us/step - loss: 231.4321 - acc: 0.0375\n",
      "Epoch 15/200\n",
      "17731/17731 [==============================] - 6s 310us/step - loss: 225.8953 - acc: 0.0395\n",
      "Epoch 16/200\n",
      "17731/17731 [==============================] - 5s 303us/step - loss: 226.9515 - acc: 0.0383\n",
      "Epoch 17/200\n",
      "17731/17731 [==============================] - 5s 286us/step - loss: 226.5417 - acc: 0.0391\n",
      "Epoch 18/200\n",
      "17731/17731 [==============================] - 5s 287us/step - loss: 221.5148 - acc: 0.0385\n",
      "Epoch 19/200\n",
      "17731/17731 [==============================] - 5s 307us/step - loss: 223.2503 - acc: 0.0376\n",
      "Epoch 20/200\n",
      "17731/17731 [==============================] - 5s 293us/step - loss: 223.0791 - acc: 0.0393\n",
      "Epoch 21/200\n",
      "17731/17731 [==============================] - 5s 286us/step - loss: 212.7933 - acc: 0.0400\n",
      "Epoch 22/200\n",
      "17731/17731 [==============================] - 5s 302us/step - loss: 212.1049 - acc: 0.0401\n",
      "Epoch 23/200\n",
      "17731/17731 [==============================] - 6s 351us/step - loss: 212.5962 - acc: 0.0390\n",
      "Epoch 24/200\n",
      "17731/17731 [==============================] - 5s 308us/step - loss: 213.0990 - acc: 0.0376\n",
      "Epoch 25/200\n",
      "17731/17731 [==============================] - 5s 266us/step - loss: 203.8502 - acc: 0.0376\n",
      "Epoch 26/200\n",
      "17731/17731 [==============================] - 5s 261us/step - loss: 205.1525 - acc: 0.0390\n",
      "Epoch 27/200\n",
      "17731/17731 [==============================] - 5s 262us/step - loss: 202.8091 - acc: 0.0374\n",
      "Epoch 28/200\n",
      "17731/17731 [==============================] - 5s 309us/step - loss: 201.2508 - acc: 0.0368\n",
      "Epoch 29/200\n",
      "17731/17731 [==============================] - 6s 323us/step - loss: 207.0549 - acc: 0.0434\n",
      "Epoch 30/200\n",
      "17731/17731 [==============================] - 8s 429us/step - loss: 199.9506 - acc: 0.0404\n",
      "Epoch 31/200\n",
      "10112/17731 [================>.............] - ETA: 2s - loss: 200.4908 - acc: 0.0405"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ddf595946e02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Train the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodelRUL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Create the model\n",
    "modelRUL = RULmodel(X_train.shape[1])\n",
    "\n",
    "#Compile the model.\n",
    "modelRUL.compile(optimizer = \"adam\", loss = \"mean_squared_error\", metrics = [\"accuracy\"])\n",
    "\n",
    "#Train the model.\n",
    "modelRUL.fit(x = X_train, y = y_train, epochs = 200, batch_size = 128)\n",
    "\n",
    "#Evaluate the model\n",
    "preds = modelRUL.evaluate(x = X_test, y = y_test)\n",
    "\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
